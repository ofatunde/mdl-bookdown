--- 
title: "Introduction"
site: bookdown::bookdown_site
output: bookdown::gitbook
---
--- 
title: "Introduction"
site: bookdown::bookdown_site
output: bookdown::gitbook
---

# Introduction
Many humanitarian and development organizations produce large amounts of data as part of their operations. UNHCR (the United Nations High Commissioner for Refugees, also known as the UN Refugee Agency) is no exception. As an output of a collaboration with the World Bank, much of the household survey data that UNHCR collects on the wellbeing and living conditions of refugees and internally displaced persons is now accessible through UNHCR microdata. The analysis of this data, which can provide hitherto inaccessible insight into the lives of those who have been forcibly displaced, can inform research, humanitarian operations, development programs and the development of policy. However, without a formal system of microdata citation, there is currently limited shared visibility into the outputs generated from this data.

Understanding UNHCR microdata use in more depth will have three principal benefits. First, it will allow for a more comprehensive assessment of the impact of data production and dissemination. Second, it would generate insight into which kinds of data are being used most effectively, and this information could be used to inform data collection and dissemination strategies. Third, it could be used to refine the ways in which data is made discoverable and accessible.

<!--chapter:end:index.Rmd-->

# Context

The [UNHCR Microdata Library](https://microdata.unhcr.org/index.php/home "MDL Homepage") is a public resource, and there are opportunities to align its functionality with the needs of the research community.

We want to 1) better understand the impact it’s having, and 2) optimize it and understand the needs of the research community (what they need and why) so we can better support them in future.

The library houses over 500 datasets, many of which are individual waves of a repeated survey. Many surveys are conducted in multiple waves, which differ by year, geography, or context. Location can be at the national, regional, or camp level.


The UNHCR/GDS Microdata Curation Team currently tracks usage of datasets through a semi-manual process.

This process is time-consuming, and also likely to miss entries due to being ad hoc.

The goal of this project was to automate and extend the current procedure to increase efficiency and provide deeper insight.

<!--chapter:end:01-context.Rmd-->

# Proposed Solution
Our proposed solution is an automated process which generates outputs at two levels.

## Dataset-level outputs

* List of citing papers
* Distribution of fields of study
* Distribution of nature of use

## Library-level outputs

* Network of citing authors
* Network of citing disciplines


```{note}
Citing disciplines depend on categories defined by external institutions.
```

We hope that this approach will provide value to multiple stakeholders, such as: 

| **Audience** 	| **Benefit** 	|
|---	|---	|
| UNHCR/ GDS Microdata Curation Team 	| Automated procedure saves time and identifies citations that would otherwise be missed, and usage statistics will give the team objective information on which datasets are useful 	|
| UNHCR Operations 	| Usage statistics and network analysis may illuminate usage gaps (e.g. additional fields that would be helpful to include in surveys) 	|
| Prospective MDL users 	| Tool can provide insight into how our data has been used in the past (and thus what is possible for their own work) 	|



<!--chapter:end:02-solution.Rmd-->

# Methods

This paper describes a methodology for systematically monitoring the use of UNHCR microdata. We develop a workflow for searching literature repositories with an initial focus on academic research, but with plans to include “grey literature” in the future. We take a three-step approach to producing a comprehensive and informative list of the papers that reference a particular dataset. 

First, for a given dataset, we begin by combining information from the metadata fields available for the study and using these terms to form multiple and overlapping search queries; these include the primary authoring organization, the year of the survey, and the country in which the study was carried out. 

Second, we use the generated query strings to generate sets of search results for each UNHCR microdata set from Semantic Scholar.

Third, once a list of possible references has been compiled for each dataset, we compute for each reference a measure of its relevance to the dataset in question using Natural Language Processing (NLP). We first generate a list of topics contained in each reference using the topic modeling and analysis tools made available by [NLP4Dev](https://www.nlp4dev.org/). To classify each possible reference by relevance to a given dataset, we consider 1) the share of identified topics related to forced displacement, 2) the set of countries mentioned in the reference, and 3) the frequency of a curated set of key-word tags related to forced displacement. 

Below we summarize the three primary methods that were used throughout this process. Each is described in greater detail in a link on the left side of the page.

|  	| **Step** 	| **Input** 	| **Output** 	| **Benefit** 	|
|---	|:---:	|:---:	|:---:	|:---:	|
| 1 	| [Query generation](methods/query-generation.md) 	| Metadata for an individual dataset 	| List of query strings 	| Information about topics and countries included, field of study and nature of use 	|
| 2 	| [Semantic search](methods/semantic-search.md) 	| One or more query strings for each dataset 	| List(s) of citing papers 	| Information about topics and countries included, field of study and nature of use 	|
| 3 	| [Topic modeling/ sentiment analysis](methods/topic-modeling-and-sentiment-analysis.md) 	| Text of individual citing papers 	| Information about topics and countries included, field of study and nature of use 	| We can determine how the dataset is used in each matching publication without resorting to manual review/analysis 	|

## Tools
We use two external tools: APIs powered by Semantic Scholar and NLP4Dev. We load corpuses from both and store them in [Google Cloud Storage](https://console.cloud.google.com/storage/browser/mdl-explorer-app;tab=objects?project=unhcr-microdata&prefix=&forceOnObjectsSortingFiltering=false).

## Citations

The following papers informed our approach: 

## Query Generation
We begin by generating a set of search queries based on the metadata for each dataset.

The full query creation notebook can be found [here](https://github.com/ofatunde/mdl-explorer-app/blob/main/notebooks/Semantic_Scholar_NLP4DEV.ipynb).

We start with simple methods of combining the metadata. The key metadata components that we use as an input into queries are:
* Core survey name
* Survey abbreviation (e.g., VaSyR)
* Lead organization (e.g., UNHCR) - short or long versions may be used
* Category (e.g., Socioeconomic Asessment of Refugees)
* Country
* Year  
For our initial exploration, we define six simple query types which represent different combinations of the information above :

* __Query type 1:__ lead_org_short + year + name + shortcode
* __Query type 2:__ lead_org_short + year + name + full_name
* __Query type 3:__ lead_org_short + year + name + shortcode_fullname
* __Query type 4:__ lead_org_long + year + name + shortcode
* __Query type 6:__ lead_org_long + year + name + shortcode_fullname

Defining query structure is an important step, as the search output can be sensitive to the contents of the query used for the search. For example, the six queries above return different number of results if entered into Semantic Scholar. 

The table below shows the number of results returned by Semantic Scholar for each dataset.
```{code-cell} ipython3
:tags: ["output_scroll","hide-input"]
import pandas as pd
df = pd.read_csv("../data/semantic_scholar_query_results_with_web_count.csv")
print(df)
```

Each of these query strings is subsequently passed into the Semantic Scholar API. The following function generates a single query which can serve as an argument into another function.

```{code-cell}
# This function takes in the name of our dataframe, the id of the data set and the query number and returns the query.
def query_finder(df_name: pd.DataFrame, dataset_id: int = 189, query_number:int =1 )->str:
    """ Function takes in the query dataframe, the dataset ID and the query type number and retuns the query itself"""
    df_indexid = df_name.set_index('id')
    query = df_indexid.loc[189][f"query_type{query_number}"]
    return query 
```

Future work will draw on more sophisticated methods of constructing queries.

### Next steps
After generating these queries, we treat them as inputs to the Semantic Scholar API. See [the next section](methods/semantic-search.md) for more details.


### Citations

The following papers informed our approach: 


## Semantic search
We made use of the [Semantic Scholar API](https://www.semanticscholar.org/product/api). For each of the query strings generated in the last section, we pass them to the ["Paper Lookup" API endpoint](https://api.semanticscholar.org/api-docs/graph#tag/Paper-Data), which we access through the [SemanticScholar Python library](https://github.com/danielnsilva/semanticscholar) using the following call:

```{code-cell}
sch = SemanticScholar()
query_type1 = []
for i in dfq["query_type1"]:
    results = sch.search_paper(i)
    query_type1.append(results)
```
The output is an object which includes (among others) the following information:
* Dataset ID
* Paper IDs (includes SS's internal ID, as well as external IDs such as DOI, arXiv, Pubmed, etc. where available)
* Title
* Authors
* Publication venue
* Publication year
* Field of Study
* Count of inbound and outbound citations
* URL
* Abstract
* PDF availability (indicates whether the full PDF is included in the corpus, and whether the abstract, body, and bibliographic entries are available in parsedt format)

We extract abstracts from this output, giving us an abstract for each dataset-query-paper combination where an abstract is available. The abstract, together with the indicator of PDF availability, serves as an input into our topic modeling procedure (see the next section.)

```{code-cell}
def semantic_search(query:str)->pd.DataFrame:
    """ This function takes in a query (from the query_finder function) and returns a dataframe of abtracts and corpus IDS"""
    sch = SemanticScholar()
    results = sch.search_paper(query)
    
    #metadata
    ### Add title
    abstracts = [item.abstract for item in results]
    identifiers =  [item.externalIds for item in results]
    API_corpus_ID = [i.get("CorpusId") for i in identifiers] # we can return whatever we want here.     
    return pd.DataFrame(list(zip(abstracts, API_corpus_ID)), columns = ['abstract', 'corpus_id'])
```

Where available, we also parse PDF content. When we parse a PDF, the output is an object which includes (among others) the following information:
* Paper ID
* PDF hash
* Abstract (if available)
* Body text (if available)
* Bibliographic entries

% From the Semantic Scholar API, we get the datasets labeled **“query_typex_abs,”** where the x  in the name = 1,2,3,4,5,6 refers to the query type. These datasets contain the following variables: **paper title, abstract, corpus ID (also called paper ID), author IDs, Digital Object identifiers (DOIs), publication year,** the **journal**  where the paper was published, and the **dataset ID**. These metadata datasets will be merged with the metadata datasets we get from the NLP4DEV API website.

### Citations

The following papers informed our approach: 


## Topic Modeling and Sentiment Analysis
We imported the documents available in the Semantic Scholar and NPL4Dev corpuses.

We make use of the [NLP4Dev API] (https://www.nlp4dev.org/content-api). Depending on whether the paper contents are available in parsed or PDF formay, we use either the "Analyze Text" or the "Analyze File" endpoint. In both cases we make use of the Mallet topic model, which allows us to use the abstract or full text of a paper as an input and retrieve information about the paper content as an output.

% ```{code-cell}
% def analyze_text(input_text:str)->pd.DataFrame:
%    url = "https://www.nlp4dev.org/nlp/models/mallet/analyze_text"
%    text = input_text
%    payload = { "model_id": "6fd8b418cbe4af7a1b3d24debfafa1ee", "text": text }
%    headers = { 'Content-Type': 'application/x-www-form-urlencoded',}
%    response = requests.request("POST", url, headers=headers, data=payload) # This returns a dictionary in string form
%
%    # Get metadata from the NLP4DEV API
%    resp = eval(response.text) # this is how we recover the dictionary from the string
%
%    #use the keys of the dictionaries to get the metadata. We have a problem here because the values are lists of different 
%    #lengths which is hard to convert to dataframes.
%
%    dataframe = pd.DataFrame(resp.get('doc_topic_words'))
%    country_counts = resp.get('country_counts')
%    country_details = resp.get('country_details')
%    country_groups = resp.get('country_groups')
%    tag_counts = resp.get('jdc_tag_counts')
%    return dataframe, list([country_counts, country_details, country_groups, tag_counts])
% ```

We run the abstracts or PDF text from the Semantic Scholar results through the NLP4DEV API, which returns the following information:
%We convert the response objects into three types of datasets: **“NLP4DEV_qx”**, **“topic_importance_querytypex”**, and **“nlpdev_queryx"**. **“NLP4DEV_qx”** datasets contain the metadata from the NLP4DEV API: 
* country counts
* country group
* JDC tags (a list of phrases defined by NLP4Dev as being related to forced displacement)
* the tag counts
* dataset ID
* Corpus ID 
* the country that is mentioned the most. 

The JDC tags include the following: 
* "asylum seeker"
* "climate refugee"
* "country of asylum"
* "exile"
* " forced displacement"
* "host community"
* "internally displaced population"
* "ocha", "population of concern"
* "refugee"
* " refugee camp"
* "repatriate"
* "resetlement area"
* "returnee"
* "stateless"
* "unhcr"

% **“nlpdev_queryx"** datasets containing the raw data dump from the NLP4DEV API. Basically, **“nlpdev_queryx"**is the raw version of **"NLP4DEV_qx"**. These datasets can be found in the NLP4DEV datasets folder on the cloud.



```{code-cell}

def NLP4Dev(abstract_or_fulltext,text_source):

    if(abstract_or_fulltext == 'PDF'):
        #extract output using "Analyze_file" endpoint
        url = "https://www.nlp4dev.org/nlp/models/mallet/analyze_file"
        payload={"model_id": "6fd8b418cbe4af7a1b3d24debfafa1ee"}
        files=[('file',('mdl-explorer-app/data/02637758211070565.pdf',open('data/02637758211070565.pdf','rb'),'application/pdf'))]
        headers = {}
        response = requests.request("POST", url, headers=headers, data=payload, files=files)
        resp = eval(response.text)
        topic_distribution = resp.get('doc_topic_words')
        country_distribution = resp.get('country_details')
        tag_distribution = resp.get('jdc_tag_counts')
    elif(abstract_or_fulltext == 'parsed_PDF'):
        #extract output using "Analyze_text" endpoint on full extracted text
        url = "https://www.nlp4dev.org/nlp/models/mallet/analyze_text"
        payload = { "model_id": "6fd8b418cbe4af7a1b3d24debfafa1ee", "text": text_source }
        headers = { 'Content-Type': 'application/x-www-form-urlencoded',}
        response = requests.request("POST", url, headers=headers, data=payload)
        resp = eval(response.text)
        topic_distribution = resp.get('doc_topic_words')
        country_distribution = resp.get('country_details')
        tag_distribution = resp.get('jdc_tag_counts')
    elif(abstract_or_fulltext == 'abstract_only'):
        #extract output using "Analyze_text" endpoint on abstract text
        url = "https://www.nlp4dev.org/nlp/models/mallet/analyze_text"
        payload = { "model_id": "6fd8b418cbe4af7a1b3d24debfafa1ee", "text": text_source }
        headers = { 'Content-Type': 'application/x-www-form-urlencoded',}
        response = requests.request("POST", url, headers=headers, data=payload)
        resp = eval(response.text)
        topic_distribution = pd.DataFrame(resp.get('doc_topic_words'))
        country_distribution = pd.DataFrame(resp.get('country_details'))
        tag_distribution = pd.DataFrame(resp.get('jdc_tag_counts'))
        topic_39_percentage = abs(topic_distribution.value[2])
       
        if len(country_distribution) > 0:
            df_indexid = country_distribution.set_index('count')
            majority_country = df_indexid.loc[max(country_distribution['count'])]['name']
        else:
            majority_country = 'NONE'
        
        jdc_tag_count = len(tag_distribution)

        
        return topic_distribution , country_distribution , tag_distribution , majority_country , topic_39_percentage, jdc_tag_count
```

\[ include a sample of text showing topic distribution \]

We use the outputs from the NLP4Dev to calculate a "relevance" score for each dataset-paper compbination

```{code-cell}
def calculate_relevance(ref,dataset, topic_39_percentage, jdc_tag_count,majority_country,relevance_threshold, paper_title = None):
    #We should define ref as a global variable if the app will always be reading from that one file
    # removed the paper titles argument because we are not using it anywhere in the function. Check with Mureji.

    dataset_country = ref.loc[ref['id'] == dataset, 'nation'].iloc[0]
    dataset_title = ref.loc[ref['id'] == dataset, 'title'].iloc[0]
    if((topic_39_percentage > relevance_threshold)and (majority_country == dataset_country)):
        relevance = 1
    elif((topic_39_percentage > relevance_threshold)and (majority_country != dataset_country)):
        relevance = 0.5
    else:
        relevance = 0
        
    final_output = pd.DataFrame(list(zip([dataset_title],[relevance]
                    ,[topic_39_percentage]
                    ,[jdc_tag_count])), columns = ["Dataset Name","Relevance","topic 39 Percentage", "JDC Tag Counts"])  
    return final_output
```

% “topic_importance_querytypex” datasets contain the topic importance. The default number of topics returned from the NLP4DEV API is 10.

The outputs of this relevance function are returned to users in our [user tool](implementation-options.md).

% We merge the semantic scholar metadata(**“query_typex_abs.csv”**) and the metadata (**“NLP4DEV_qx”**) files we get from running the abstracts through the NLP4DEV API. The datasets labeled **“ss_NLP4_queryx_merge”** and **“qx_ss_nlpdev_master”** are the results of different types or merges from the two metadata sources. If in doubt, use the **“qx_ss_nlpdev_master”** datasets.

% **“qx_datasets”** contain the  datasets that returned queries from the NLP4DEV API for each query. This returns the datasets that do not have HTML tags. 
%**“qx_qx_sample_streamlit_demo”** and **"qx_qx_demo_short"** contain the following variables: **title**, **abstract**, **dataset number**, and **query number**.
%**“qx_qx_sample_results”** datasets contain the information we need to display in the app. It contains the following variables: **dataset title**, **paper title**, **relevance**, **JDC tags**, **value**, and **topic percentages**.


### Citations

The following papers informed our approach:


## Network Analysis

The final portion of our methodology allows us to obtain higher-level insights about the network of authors/papers making use of UNHCR microdata.

{cite}`ValenzuelaEscarcega2015IdentifyingMC` have worked on identifying "meaningful" citations Researchers such as {cite}`Cohan2019` and {cite}`teufel-etal-2006-annotation` have developed methods for inferring "citation intent", or information about _how_ a paper references another paper (or in our case, a dataset). {cite}`Jurgens2018` extend this to use citations as a lens through which to understand the evolution of a given scientific field.

<!--chapter:end:03-method.Rmd-->


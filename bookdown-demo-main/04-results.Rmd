# Results
In this section we describe the output of the pipeline that was developed in the Methods section. This section has two components:

## Model Output
We refer to each combination of query selection, results retrieval, and relevance scoring method, as a "model". The output of each model is therefore the list of results returned.

Each model returns a list of papers with the following information:

* Dataset Title
* Paper Title
* Binary relevance
* Continunous relevance score
* Number of JDC tags mentioned
* Percentage of paper dedicated to forced displacement

## Evaluating Model performance
Because each model has unique inputs and outputs, we can evaluate each model on some relevant performance metric. Our main aim is to evaluate how effective each model is at returning the highest number of "potentially relevant results that are subsequently confirmed to be truly relevant.

There are several metrics that we could use to evaluate the performance of each iteration of the analysis pipeline, each of which is uniquely defined by 1) the query generation method, 2) our choice of topic model, and 3) the specification of the relevance function.

We considered several options, which are listed below.

|  	| **Evaluation metric** 	| **Definition** 	| **Benefits** 	| **Other considerations** 	|
|---	|:---:	|:---:	|:---:	|:---:	|
| 1 	| Precision 	| xx 	| xx 	| xx 	|
| 2 	| Recall 	|  xx 	| xx 	| xx 	|
| 3 	| Percent unique 	| xx 	| xx 	| xx 	|


Our primary metric will be XXX based on the work of YYY.